{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ6wc2HE0pke"
      },
      "source": [
        "# **Experiment Notebook**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFVpE17Ahezu"
      },
      "source": [
        "---\n",
        "## 0. Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM39DExClCc5"
      },
      "source": [
        "### 0.b Disable Warnings Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6FneOmBfka9G"
      },
      "outputs": [],
      "source": [
        "# Do not modify this code\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgTrMfyylVLf"
      },
      "source": [
        "### 0.c Install Additional Packages\n",
        "\n",
        "> If you are using additional packages, you need to install them here using the command: `! pip install <package_name>`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXFKfa2tp1ch"
      },
      "source": [
        "### 0.d Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBEAwdncnlAx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q1Bzcejvfpm"
      },
      "source": [
        "---\n",
        "## B. Experiment Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_tile(size=\"h3\", key=None, value=None):\n",
        "    \"\"\"\n",
        "    Prints a formatted tile with a given size, key, and value.\n",
        "    Args:\n",
        "        size (str): HTML heading size, e.g., \"h3\".\n",
        "        key (str): Unique identifier for the tile.\n",
        "        value (str): Content to display in the tile.\n",
        "    \"\"\"\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    html = f'<{size} id=\"{key}\">{value}</{size}>'\n",
        "    display(HTML(html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qng9pUoU21Kp"
      },
      "outputs": [],
      "source": [
        "\n",
        "experiment_hypothesis = \"\"\"\n",
        "The hypothesis to test in this project is: **\"Student poor performance can be accurately predicted using key academic, behavioral, and socio-economic features such as GPA, study hours, social media usage, and attendance rates.\"** The question seeks to determine how well these factors correlate with and influence academic success, enabling classification into performance categories like \"Excellent,\" \"Good,\" \"Average,\" and \"Poor.\"\n",
        "\n",
        "This hypothesis is worthwhile because accurate predictions can allow institutions to identify at-risk students early, provide tailored interventions, and enhance academic outcomes. It also helps optimize resource allocation by focusing efforts on students who need the most support. By understanding the significant predictors of performance, institutions can make data-driven decisions, enhancing both individual student success and overall educational quality. This insight provides a robust foundation for sustainable improvements in academic strategies.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "3F9fpCzf21YI",
        "outputId": "170ce199-a281-4a59-d445-53eb65f242d4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"experiment_hypothesis\">\n",
              "The hypothesis to test in this project is: **\"Student poor performance can be accurately predicted using key academic, behavioral, and socio-economic features such as GPA, study hours, social media usage, and attendance rates.\"** The question seeks to determine how well these factors correlate with and influence academic success, enabling classification into performance categories like \"Excellent,\" \"Good,\" \"Average,\" and \"Poor.\"\n",
              "\n",
              "This hypothesis is worthwhile because accurate predictions can allow institutions to identify at-risk students early, provide tailored interventions, and enhance academic outcomes. It also helps optimize resource allocation by focusing efforts on students who need the most support. By understanding the significant predictors of performance, institutions can make data-driven decisions, enhancing both individual student success and overall educational quality. This insight provides a robust foundation for sustainable improvements in academic strategies.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='experiment_hypothesis', value=experiment_hypothesis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ke5AG7A21dx"
      },
      "outputs": [],
      "source": [
        "\n",
        "experiment_expectations = \"\"\"\n",
        "The expected outcome of the experiment is to create a model that predicts student performance categories—\"Excellent,\" \"Good,\" \"Average,\" \"Poor.\"\n",
        "### Possible Scenarios:\n",
        "1. **Best Case**: The model's performance improves, reaching over 80% F1 score with balanced metrics across all classes, enabling effective interventions for students.\n",
        "2. **Moderate Case**: Metrics remain skewed toward dominant classes (\"Poor\"), and minority classes like \"Excellent\" and \"Good\" have low recall, requiring adjustments in features or algorithms.\n",
        "3. **Worst Case**: The model fails to generalize, achieving F1 score below 50%, leading to misallocated resources and diminished institutional trust.\n",
        "\n",
        "These outcomes highlight the need for refining the model to improve predictions for minority classes while leveraging its strength in dominant categories.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Aal6ECC821nJ",
        "outputId": "b323dd56-fa00-47ad-dec0-cc60ae4bb9c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"experiment_expectations\">\n",
              "The expected outcome of the experiment is to create a model that predicts student performance categories—\"Excellent,\" \"Good,\" \"Average,\" \"Poor.\"\n",
              "### Possible Scenarios:\n",
              "1. **Best Case**: The model's performance improves, reaching over 80% F1 score with balanced metrics across all classes, enabling effective interventions for students.\n",
              "2. **Moderate Case**: Metrics remain skewed toward dominant classes (\"Poor\"), and minority classes like \"Excellent\" and \"Good\" have low recall, requiring adjustments in features or algorithms.\n",
              "3. **Worst Case**: The model fails to generalize, achieving F1 score below 50%, leading to misallocated resources and diminished institutional trust.\n",
              "\n",
              "These outcomes highlight the need for refining the model to improve predictions for minority classes while leveraging its strength in dominant categories.\n",
              "\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='experiment_expectations', value=experiment_expectations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0zsEPshwy1K"
      },
      "source": [
        "---\n",
        "## C. Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NKgOzSn-w0eq"
      },
      "outputs": [],
      "source": [
        "# Do not modify this code\n",
        "try:\n",
        "  X_train = pd.read_csv('../data/processed/X_train.csv')\n",
        "  y_train = pd.read_csv('../data/processed/y_train.csv')\n",
        "\n",
        "  X_val = pd.read_csv('../data/processed/X_val.csv')\n",
        "  y_val = pd.read_csv('../data/processed/y_val.csv')\n",
        "\n",
        "  X_test = pd.read_csv('../data/processed/X_test.csv')\n",
        "  y_test = pd.read_csv('../data/processed/y_test.csv')\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NCwQQFkU3v5"
      },
      "source": [
        "---\n",
        "## D. Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Boo6bl0YhpM_",
        "outputId": "7ad68250-8c8e-433c-af70-438b7187c6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "student_id                       -0.416043\n",
            "age                              -0.046171\n",
            "hsc_year                          0.188994\n",
            "current _semester                -0.184882\n",
            "study_hours                       0.236444\n",
            "social_media_hours               -0.443580\n",
            "average_attendance               -0.242111\n",
            "skills_development_hours         -0.048071\n",
            "previous_gpa                      0.687628\n",
            "current_gpa                      -0.542631\n",
            "completed_credits                -0.254523\n",
            "house_income                     -0.259291\n",
            "gpa_consistency                   0.332170\n",
            "social_media_impact               0.464983\n",
            "income_academic_score            -0.261522\n",
            "english_proficiency_encoded      -0.063948\n",
            "birth_country_AU                 -0.076070\n",
            "birth_country_BR                  0.007581\n",
            "birth_country_CA                 -0.033834\n",
            "birth_country_IE                  0.050013\n",
            "birth_country_IN                  0.051579\n",
            "birth_country_NZ                 -0.026911\n",
            "birth_country_PH                  0.005309\n",
            "birth_country_TH                  0.069651\n",
            "birth_country_US                  0.050013\n",
            "birth_country_ZA                  0.061342\n",
            "scholarship_No                   -0.329113\n",
            "scholarship_Yes                   0.329113\n",
            "university_transport_No          -0.045174\n",
            "university_transport_Yes          0.045174\n",
            "learning_mode_Offline             0.127438\n",
            "learning_mode_Online             -0.127438\n",
            "on_probation_No                   0.292227\n",
            "on_probation_Yes                 -0.292227\n",
            "is_suspended_No                   0.044970\n",
            "is_suspended_Yes                 -0.044970\n",
            "relationship_Engaged             -0.023254\n",
            "relationship_In a relationship   -0.194657\n",
            "relationship_Married             -0.127641\n",
            "relationship_Single               0.246625\n",
            "target                            1.000000\n",
            "Name: target, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_data = pd.concat([X_train, y_train], axis=1)\n",
        "correlation_matrix = train_data.corr()\n",
        "\n",
        "correlation_with_y_train = correlation_matrix['target']\n",
        "\n",
        "# Print the correlations\n",
        "print(correlation_with_y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHHcoT37hvan",
        "outputId": "7f9effb7-ea58-4e67-caa6-a23a13dee865"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['previous_gpa', 'current_gpa', 'social_media_impact',\n",
              "       'social_media_hours', 'student_id', 'gpa_consistency',\n",
              "       'scholarship_Yes', 'scholarship_No'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted_correlation = correlation_with_y_train.drop('target').abs().sort_values(ascending=False)\n",
        "\n",
        "# Select the top 8 columns\n",
        "top_8_columns = sorted_correlation.head(8).index\n",
        "top_8_columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HieJHxthvXD",
        "outputId": "656054a6-f1dc-4855-9163-bbe922293b1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['student_id', 'age', 'hsc_year', 'current _semester', 'study_hours',\n",
              "       'social_media_hours', 'average_attendance', 'skills_development_hours',\n",
              "       'previous_gpa', 'current_gpa', 'completed_credits', 'house_income',\n",
              "       'gpa_consistency', 'social_media_impact', 'income_academic_score',\n",
              "       'english_proficiency_encoded', 'birth_country_AU', 'birth_country_BR',\n",
              "       'birth_country_CA', 'birth_country_IE', 'birth_country_IN',\n",
              "       'birth_country_NZ', 'birth_country_PH', 'birth_country_TH',\n",
              "       'birth_country_US', 'birth_country_ZA', 'scholarship_No',\n",
              "       'scholarship_Yes', 'university_transport_No',\n",
              "       'university_transport_Yes', 'learning_mode_Offline',\n",
              "       'learning_mode_Online', 'on_probation_No', 'on_probation_Yes',\n",
              "       'is_suspended_No', 'is_suspended_Yes', 'relationship_Engaged',\n",
              "       'relationship_In a relationship', 'relationship_Married',\n",
              "       'relationship_Single'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting the list of column in the X_train df\n",
        "features_list = X_train.columns\n",
        "features_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbKAQPiihvUY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHaIRLAj2_fj"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_selection_explanations = \"\"\"### Feature Selection Rationale\n",
        "*note all the numerical value are approximate and are subject to change due to randomness*\n",
        "#### **Selected Features**\n",
        "The features chosen for the model—`study_hours`, `social_media_hours`, `previous_gpa`, `current_gpa`, and `on_probation_No`—have strong correlations with the target variable and high predictive relevance. For example:\n",
        "- **`Previous_gpa`**: With a correlation of 0.688, this feature is the strongest predictor of academic performance, reflecting prior achievements.\n",
        "- **`Current_gpa`**: This complements `previous_gpa` with a correlation of -0.543, offering insights into ongoing trends.\n",
        "- **`Social_media_hours`**: Correlated at -0.443, this helps capture behavioral patterns that may negatively impact academic outcomes.\n",
        "- **`Study_hours`**: Correlated at 0.236, this feature provides direct input on time invested in academics.\n",
        "- **`On_probation_No`**: Adds categorical context related to academic status, enhancing predictive accuracy.\n",
        "\n",
        "#### **Reasons for Removing Features**\n",
        "Other features were excluded due to:\n",
        "1. **Weak Correlations**:\n",
        "   - Features like `age` (-0.046) and `skills_development_hours` (-0.048) show negligible relationships with the target variable, contributing little to the model's performance.\n",
        "\n",
        "2. **High Cardinality and Redundancy**:\n",
        "   - Features such as `birth_country` and `relationship_status` add complexity without significant predictive value.\n",
        "\n",
        "3. **Potential Noise**:\n",
        "   - Features like `house_income` (-0.259) and `average_attendance` (-0.242) are less directly connected to academic outcomes and could introduce unnecessary noise into the model.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "GXCmXWx62_kZ",
        "outputId": "ef1eac74-e808-4b2e-8736-b7ed057bd217"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"feature_selection_explanations\">### Feature Selection Rationale\n",
              "*note all the numerical value are approximate and are subject to change due to randomness*\n",
              "#### **Selected Features**\n",
              "The features chosen for the model—`study_hours`, `social_media_hours`, `previous_gpa`, `current_gpa`, and `on_probation_No`—have strong correlations with the target variable and high predictive relevance. For example:\n",
              "- **`Previous_gpa`**: With a correlation of 0.688, this feature is the strongest predictor of academic performance, reflecting prior achievements.\n",
              "- **`Current_gpa`**: This complements `previous_gpa` with a correlation of -0.543, offering insights into ongoing trends.\n",
              "- **`Social_media_hours`**: Correlated at -0.443, this helps capture behavioral patterns that may negatively impact academic outcomes.\n",
              "- **`Study_hours`**: Correlated at 0.236, this feature provides direct input on time invested in academics.\n",
              "- **`On_probation_No`**: Adds categorical context related to academic status, enhancing predictive accuracy.\n",
              "\n",
              "#### **Reasons for Removing Features**\n",
              "Other features were excluded due to:\n",
              "1. **Weak Correlations**:\n",
              "   - Features like `age` (-0.046) and `skills_development_hours` (-0.048) show negligible relationships with the target variable, contributing little to the model's performance.\n",
              "\n",
              "2. **High Cardinality and Redundancy**:\n",
              "   - Features such as `birth_country` and `relationship_status` add complexity without significant predictive value.\n",
              "\n",
              "3. **Potential Noise**:\n",
              "   - Features like `house_income` (-0.259) and `average_attendance` (-0.242) are less directly connected to academic outcomes and could introduce unnecessary noise into the model.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='feature_selection_explanations', value=feature_selection_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-nNSpJK0Rgu"
      },
      "source": [
        "---\n",
        "## E. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylQxO-4g03qH"
      },
      "source": [
        "### E.3 Data Transformation Zscore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VymIZpS1Ch7",
        "outputId": "6abcefea-781d-48d8-f4dd-bbeaa70fd0e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Normalized Data:\n",
            "      student_id       age  hsc_year  current _semester  study_hours  \\\n",
            "0     -0.495645  1.738179  0.708183          -1.602195    -0.778038   \n",
            "1     -0.489071 -0.684350 -0.220882          -1.602195    -0.778038   \n",
            "2     -1.399514 -0.684350 -0.220882          -1.602195    -0.778038   \n",
            "3     -0.719147  0.123160 -0.220882          -1.602195     0.247891   \n",
            "4     -0.784883 -0.684350 -0.220882          -1.602195    -0.059159   \n",
            "..          ...       ...       ...                ...          ...   \n",
            "690    0.378644  1.738179  0.708183           1.450651    -0.059159   \n",
            "691    0.365497 -0.684350 -0.220882           1.450651    -1.704833   \n",
            "692    0.927540 -0.684350  0.708183           1.450651     1.024818   \n",
            "693   -1.350213  1.738179  2.515454           1.450651    -0.778038   \n",
            "694   -1.356786  0.930670  0.708183           1.450651     1.455002   \n",
            "\n",
            "     social_media_hours  average_attendance  skills_development_hours  \\\n",
            "0              1.912304           -1.206113                 -1.019838   \n",
            "1              1.619913           -1.206113                 -1.019838   \n",
            "2              1.293047           -1.206113                  1.596554   \n",
            "3             -1.428898           -0.709872                 -1.019838   \n",
            "4             -0.011282           -1.206113                  0.066063   \n",
            "..                  ...                 ...                       ...   \n",
            "690           -0.011282            0.683252                  0.066063   \n",
            "691           -0.011282            1.550301                  0.908353   \n",
            "692            0.922478           -1.206113                 -1.019838   \n",
            "693           -0.630538            0.076651                  0.066063   \n",
            "694           -0.011282            0.510598                 -1.019838   \n",
            "\n",
            "     previous_gpa  current_gpa  ...  learning_mode_Offline  \\\n",
            "0       -3.133916     3.234574  ...               0.600065   \n",
            "1       -3.133916     3.234574  ...               0.600065   \n",
            "2        1.326078    -1.340294  ...               0.600065   \n",
            "3        1.883577    -1.340294  ...               0.600065   \n",
            "4       -3.133916     3.234574  ...               0.600065   \n",
            "..            ...          ...  ...                    ...   \n",
            "690     -0.067670    -0.916946  ...               0.600065   \n",
            "691     -0.413320    -0.412187  ...               0.600065   \n",
            "692     -0.825869     0.001328  ...               0.600065   \n",
            "693      0.367179     0.214381  ...               0.600065   \n",
            "694      1.147678    -0.857380  ...               0.600065   \n",
            "\n",
            "     learning_mode_Online  on_probation_No  on_probation_Yes  is_suspended_No  \\\n",
            "0               -0.600065         0.549088         -0.549088         0.158347   \n",
            "1               -0.600065         0.549088         -0.549088         0.158347   \n",
            "2               -0.600065        -1.821200          1.821200         0.158347   \n",
            "3               -0.600065         0.549088         -0.549088         0.158347   \n",
            "4               -0.600065         0.549088         -0.549088         0.158347   \n",
            "..                    ...              ...               ...              ...   \n",
            "690             -0.600065        -1.821200          1.821200         0.158347   \n",
            "691             -0.600065        -1.821200          1.821200         0.158347   \n",
            "692             -0.600065         0.549088         -0.549088         0.158347   \n",
            "693             -0.600065         0.549088         -0.549088         0.158347   \n",
            "694             -0.600065         0.549088         -0.549088         0.158347   \n",
            "\n",
            "     is_suspended_Yes  relationship_Engaged  relationship_In a relationship  \\\n",
            "0           -0.158347             -0.114541                       -0.405159   \n",
            "1           -0.158347             -0.114541                       -0.405159   \n",
            "2           -0.158347             -0.114541                       -0.405159   \n",
            "3           -0.158347             -0.114541                       -0.405159   \n",
            "4           -0.158347             -0.114541                       -0.405159   \n",
            "..                ...                   ...                             ...   \n",
            "690         -0.158347             -0.114541                       -0.405159   \n",
            "691         -0.158347             -0.114541                       -0.405159   \n",
            "692         -0.158347             -0.114541                       -0.405159   \n",
            "693         -0.158347             -0.114541                       -0.405159   \n",
            "694         -0.158347             -0.114541                       -0.405159   \n",
            "\n",
            "     relationship_Married  relationship_Single  \n",
            "0               -0.269316             0.533534  \n",
            "1               -0.269316             0.533534  \n",
            "2               -0.269316             0.533534  \n",
            "3               -0.269316             0.533534  \n",
            "4               -0.269316             0.533534  \n",
            "..                    ...                  ...  \n",
            "690              3.713116            -1.874296  \n",
            "691             -0.269316             0.533534  \n",
            "692             -0.269316             0.533534  \n",
            "693             -0.269316             0.533534  \n",
            "694             -0.269316             0.533534  \n",
            "\n",
            "[695 rows x 40 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply Z-score normalization\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(X_train)\n",
        "\n",
        "# Convert normalized data back to DataFrame\n",
        "normalized_X_train = pd.DataFrame(normalized_data, columns=X_train.columns)\n",
        "\n",
        "# Display normalized DataFrame\n",
        "print(\"\\nNormalized Data:\\n\", normalized_X_train)\n",
        "X_train=normalized_X_train.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV45F5JWvMPN",
        "outputId": "1fb56eda-72f3-401a-b063-34fe00b1b045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Normalized Data:\n",
            "      student_id       age  hsc_year  current _semester  study_hours  \\\n",
            "0      0.502863 -1.100388  1.398646          -0.527970    -0.751946   \n",
            "1      0.159785 -0.515075  0.058091          -0.527970     0.411772   \n",
            "2      0.956323 -1.685700  1.398646          -0.527970    -1.333805   \n",
            "3      0.944390  1.240863  0.728368          -0.527970     0.993630   \n",
            "4      1.051788 -0.515075  0.058091          -0.527970     0.993630   \n",
            "..          ...       ...       ...                ...          ...   \n",
            "145   -0.556204 -1.685700  1.398646           2.689067     0.411772   \n",
            "146   -1.075296 -0.515075  1.398646           2.689067    -0.751946   \n",
            "147   -0.669569 -0.515075  0.728368           2.689067    -0.751946   \n",
            "148   -0.857516 -1.685700  0.728368           2.689067    -0.751946   \n",
            "149    0.094153  1.240863 -3.293298           3.101374    -1.333805   \n",
            "\n",
            "     social_media_hours  average_attendance  skills_development_hours  \\\n",
            "0             -1.313636            0.909897                 -0.250979   \n",
            "1              2.548521           -0.779914                 -0.250979   \n",
            "2              0.109503           -1.268378                  1.114827   \n",
            "3              0.109503           -1.268378                  1.114827   \n",
            "4              2.040581            0.971214                 -0.250979   \n",
            "..                  ...                 ...                       ...   \n",
            "145            0.109503           -1.268378                 -0.250979   \n",
            "146            0.109503           -0.494180                 -0.250979   \n",
            "147            0.109503            0.877114                 -0.250979   \n",
            "148           -1.821576           -0.779914                 -0.250979   \n",
            "149            2.977979            0.877114                  1.114827   \n",
            "\n",
            "     previous_gpa  current_gpa  ...  learning_mode_Offline  \\\n",
            "0        0.031091     1.413634  ...               0.633932   \n",
            "1        1.272015     1.171699  ...               0.633932   \n",
            "2       -0.050728     1.347652  ...               0.633932   \n",
            "3       -0.309822     0.929764  ...               0.633932   \n",
            "4       -0.623462     1.391640  ...               0.633932   \n",
            "..            ...          ...  ...                    ...   \n",
            "145     -0.078001    -1.137681  ...               0.633932   \n",
            "146      1.422016     0.335923  ...               0.633932   \n",
            "147      1.244741     0.335923  ...               0.633932   \n",
            "148      3.017489     0.313929  ...               0.633932   \n",
            "149      0.017455    -1.643545  ...              -1.577457   \n",
            "\n",
            "     learning_mode_Online  on_probation_No  on_probation_Yes  is_suspended_No  \\\n",
            "0               -0.633932         0.728431         -0.728431         0.281312   \n",
            "1               -0.633932         0.728431         -0.728431         0.281312   \n",
            "2               -0.633932         0.728431         -0.728431         0.281312   \n",
            "3               -0.633932        -1.372813          1.372813         0.281312   \n",
            "4               -0.633932         0.728431         -0.728431         0.281312   \n",
            "..                    ...              ...               ...              ...   \n",
            "145             -0.633932        -1.372813          1.372813        -3.554766   \n",
            "146             -0.633932         0.728431         -0.728431         0.281312   \n",
            "147             -0.633932         0.728431         -0.728431         0.281312   \n",
            "148             -0.633932        -1.372813          1.372813         0.281312   \n",
            "149              1.577457        -1.372813          1.372813        -3.554766   \n",
            "\n",
            "     is_suspended_Yes  relationship_Engaged  relationship_In a relationship  \\\n",
            "0           -0.281312             -0.116248                       -0.447214   \n",
            "1           -0.281312             -0.116248                       -0.447214   \n",
            "2           -0.281312             -0.116248                        2.236068   \n",
            "3           -0.281312             -0.116248                        2.236068   \n",
            "4           -0.281312             -0.116248                       -0.447214   \n",
            "..                ...                   ...                             ...   \n",
            "145          3.554766             -0.116248                       -0.447214   \n",
            "146         -0.281312             -0.116248                       -0.447214   \n",
            "147         -0.281312             -0.116248                       -0.447214   \n",
            "148         -0.281312             -0.116248                       -0.447214   \n",
            "149          3.554766             -0.116248                       -0.447214   \n",
            "\n",
            "     relationship_Married  relationship_Single  \n",
            "0               -0.414578             0.696526  \n",
            "1               -0.414578             0.696526  \n",
            "2               -0.414578            -1.435697  \n",
            "3               -0.414578            -1.435697  \n",
            "4               -0.414578             0.696526  \n",
            "..                    ...                  ...  \n",
            "145             -0.414578             0.696526  \n",
            "146             -0.414578             0.696526  \n",
            "147             -0.414578             0.696526  \n",
            "148             -0.414578             0.696526  \n",
            "149              2.412091            -1.435697  \n",
            "\n",
            "[150 rows x 40 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Apply Z-score normalization\n",
        "\n",
        "normalized_data = scaler.transform(X_test)\n",
        "\n",
        "# Convert normalized data back to DataFrame\n",
        "normalized_X_test = pd.DataFrame(normalized_data, columns=X_test.columns)\n",
        "\n",
        "# Display normalized DataFrame\n",
        "print(\"\\nNormalized Data:\\n\", normalized_X_test)\n",
        "X_test=normalized_X_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_A_ToxHvME9",
        "outputId": "ef23d421-9026-42ac-a8dc-6f3b3644a7f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Normalized Data:\n",
            "      student_id       age  hsc_year  current _semester  study_hours  \\\n",
            "0      0.536376 -0.777878  0.218604          -2.040945    -0.033080   \n",
            "1      0.538351 -1.363463  1.117308          -2.040945     0.647374   \n",
            "2      0.221599  0.393291  0.218604          -2.040945    -1.393990   \n",
            "3      0.152987 -1.363463  1.117308          -2.040945    -0.713535   \n",
            "4      0.165325 -0.777878  1.117308          -2.040945    -0.033080   \n",
            "..          ...       ...       ...                ...          ...   \n",
            "143   -0.572999  0.393291 -0.680101           1.189642    -1.393990   \n",
            "144   -0.544243  1.564461  0.218604           1.189642    -0.713535   \n",
            "145   -0.565718  0.978876 -0.680101           1.189642    -0.033080   \n",
            "146    0.667978 -0.192293 -1.578805           1.189642    -0.713535   \n",
            "147    0.710894 -0.192293 -0.680101           1.189642    -0.713535   \n",
            "\n",
            "     social_media_hours  average_attendance  skills_development_hours  \\\n",
            "0              1.996438           -1.096522                 -0.385479   \n",
            "1             -0.702126            0.987976                  1.156438   \n",
            "2              0.851482           -1.096522                 -1.156438   \n",
            "3             -0.057321            1.448785                 -1.156438   \n",
            "4             -0.057321            1.567580                 -0.385479   \n",
            "..                  ...                 ...                       ...   \n",
            "143           -0.702126           -1.096522                  0.385479   \n",
            "144           -0.702126            1.247826                  1.156438   \n",
            "145            1.496288            0.608674                  0.385479   \n",
            "146           -0.702126           -1.096522                  1.927397   \n",
            "147            0.442829            0.497615                  0.385479   \n",
            "\n",
            "     previous_gpa  current_gpa  ...  learning_mode_Offline  \\\n",
            "0       -0.844105    -1.151256  ...              -1.562645   \n",
            "1       -0.903159    -0.974163  ...               0.639940   \n",
            "2        0.691284     1.217367  ...               0.639940   \n",
            "3        0.632231     1.239504  ...               0.639940   \n",
            "4       -0.829342    -0.929890  ...              -1.562645   \n",
            "..            ...          ...  ...                    ...   \n",
            "143      0.130277     1.040274  ...               0.639940   \n",
            "144     -0.844105    -0.929890  ...               0.639940   \n",
            "145     -0.415968    -0.022286  ...               0.639940   \n",
            "146      0.971788    -0.398610  ...              -1.562645   \n",
            "147      1.798537     0.066260  ...               0.639940   \n",
            "\n",
            "     learning_mode_Online  on_probation_No  on_probation_Yes  is_suspended_No  \\\n",
            "0                1.562645         0.660979         -0.660979         0.222812   \n",
            "1               -0.639940        -1.512907          1.512907         0.222812   \n",
            "2               -0.639940         0.660979         -0.660979         0.222812   \n",
            "3               -0.639940         0.660979         -0.660979         0.222812   \n",
            "4                1.562645         0.660979         -0.660979         0.222812   \n",
            "..                    ...              ...               ...              ...   \n",
            "143             -0.639940         0.660979         -0.660979         0.222812   \n",
            "144             -0.639940        -1.512907          1.512907         0.222812   \n",
            "145             -0.639940        -1.512907          1.512907         0.222812   \n",
            "146              1.562645         0.660979         -0.660979         0.222812   \n",
            "147             -0.639940         0.660979         -0.660979         0.222812   \n",
            "\n",
            "     is_suspended_Yes  relationship_Engaged  relationship_In a relationship  \\\n",
            "0           -0.222812         -8.945682e-17                        1.379116   \n",
            "1           -0.222812         -8.945682e-17                        1.379116   \n",
            "2           -0.222812         -8.945682e-17                        1.379116   \n",
            "3           -0.222812         -8.945682e-17                        1.379116   \n",
            "4           -0.222812         -8.945682e-17                       -0.725102   \n",
            "..                ...                   ...                             ...   \n",
            "143         -0.222812         -8.945682e-17                       -0.725102   \n",
            "144         -0.222812         -8.945682e-17                        1.379116   \n",
            "145         -0.222812         -8.945682e-17                       -0.725102   \n",
            "146         -0.222812         -8.945682e-17                       -0.725102   \n",
            "147         -0.222812         -8.945682e-17                       -0.725102   \n",
            "\n",
            "     relationship_Married  relationship_Single  \n",
            "0               -0.395285            -1.041397  \n",
            "1               -0.395285            -1.041397  \n",
            "2               -0.395285            -1.041397  \n",
            "3               -0.395285            -1.041397  \n",
            "4               -0.395285             0.960249  \n",
            "..                    ...                  ...  \n",
            "143             -0.395285             0.960249  \n",
            "144             -0.395285            -1.041397  \n",
            "145             -0.395285             0.960249  \n",
            "146             -0.395285             0.960249  \n",
            "147             -0.395285             0.960249  \n",
            "\n",
            "[148 rows x 40 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Apply Z-score normalization\n",
        "\n",
        "normalized_data = scaler.transform(X_val)\n",
        "\n",
        "# Convert normalized data back to DataFrame\n",
        "normalized_X_val = pd.DataFrame(normalized_data, columns=X_val.columns)\n",
        "\n",
        "# Display normalized DataFrame\n",
        "print(\"\\nNormalized Data:\\n\", normalized_X_val)\n",
        "X_val=normalized_X_val.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68S5Eev23b1j"
      },
      "outputs": [],
      "source": [
        "\n",
        "data_transformation_3_explanations = \"\"\"Performing Z-score normalization (standardization) is crucial to ensure the dataset is optimized for machine learning algorithms:\n",
        "\n",
        "### **Why It's Important**\n",
        "1. **Centers Data**: It scales features like `study_hours` and `social_media_hours` to a mean of 0 with a standard deviation of 1, making data comparable across features.\n",
        "2. **Enhances Algorithm Efficiency**: Many models, such as Logistic Regression, perform optimally when features are standardized.\n",
        "3. **Addresses Outliers**: Z-score normalization reduces the influence of outliers, ensuring that extreme values don't disproportionately impact the model.\n",
        "\n",
        "### **Impacts**\n",
        "1. **Improved Model Accuracy**: Standardized data enables more accurate predictions by ensuring balanced contributions from all features.\n",
        "2. **Faster Convergence**: Gradient-based optimization techniques benefit significantly from normalized data, resulting in efficient model training.\n",
        "3. **Better Interpretability**: Normalized coefficients make it easier to understand the relationship between features and the target variable.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "yeT-WCIm3b4g",
        "outputId": "5dd3350a-bc43-4e36-93ab-8ed815a62e05"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"data_transformation_3_explanations\">Performing Z-score normalization (standardization) is crucial to ensure the dataset is optimized for machine learning algorithms:\n",
              "\n",
              "### **Why It's Important**\n",
              "1. **Centers Data**: It scales features like `study_hours` and `social_media_hours` to a mean of 0 with a standard deviation of 1, making data comparable across features.\n",
              "2. **Enhances Algorithm Efficiency**: Many models, such as Logistic Regression, perform optimally when features are standardized.\n",
              "3. **Addresses Outliers**: Z-score normalization reduces the influence of outliers, ensuring that extreme values don't disproportionately impact the model.\n",
              "\n",
              "### **Impacts**\n",
              "1. **Improved Model Accuracy**: Standardized data enables more accurate predictions by ensuring balanced contributions from all features.\n",
              "2. **Faster Convergence**: Gradient-based optimization techniques benefit significantly from normalized data, resulting in efficient model training.\n",
              "3. **Better Interpretability**: Normalized coefficients make it easier to understand the relationship between features and the target variable.\n",
              "\n",
              "\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='data_transformation_3_explanations', value=data_transformation_3_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S80O7okb0RIx"
      },
      "source": [
        "---\n",
        "## F. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Fhn4271gVu"
      },
      "source": [
        "### Performing \"PCA\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6wBdmYD1g6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.90)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_train = pd.DataFrame(X_train_pca)  # Convert back to DataFrame if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BJL2puT9xvcX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "X_val_pca = pca.transform(X_val)\n",
        "X_val = pd.DataFrame(X_val_pca)  # Convert back to DataFrame if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "29ee7CkbxvWZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "X_test_pca = pca.transform(X_test)\n",
        "X_test = pd.DataFrame(X_test_pca)  # Convert back to DataFrame if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftZovroG3pv8"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_engineering_3_explanations = \"\"\"Creating Principal Component Analysis (PCA) features is crucial for dimensionality reduction and improving model performance by capturing the most important variance in the dataset.\n",
        "\n",
        "### **Why It's Important**\n",
        "1. **Reduces Dimensionality**:\n",
        "   - PCA condenses the dataset into fewer components while retaining 90% of its variance, reducing computational complexity and overfitting.\n",
        "\n",
        "2. **Highlights Key Patterns**:\n",
        "   - By transforming features into orthogonal components, PCA identifies and preserves relationships in the data that may not be obvious in raw features.\n",
        "\n",
        "3. **Improves Model Efficiency**:\n",
        "   - With fewer but more informative components, the model can train faster and generalize better, particularly on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Impacts**\n",
        "1. **Enhanced Predictive Accuracy**:\n",
        "   - Eliminating irrelevant or redundant features improves the model's ability to make accurate predictions.\n",
        "\n",
        "2. **Improved Interpretability**:\n",
        "   - PCA simplifies the dataset, making it easier to understand the relationships driving the predictions.\n",
        "\n",
        "3. **Better Generalization**:\n",
        "   - Reduced dimensionality mitigates overfitting risks, ensuring consistent performance on validation and test datasets.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "xDeQtmJf3pzj",
        "outputId": "85351b72-0ab0-4d3a-ddc5-25ee8feb745b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"feature_engineering_3_explanations\">Creating Principal Component Analysis (PCA) features is crucial for dimensionality reduction and improving model performance by capturing the most important variance in the dataset.\n",
              "\n",
              "### **Why It's Important**\n",
              "1. **Reduces Dimensionality**:\n",
              "   - PCA condenses the dataset into fewer components while retaining 90% of its variance, reducing computational complexity and overfitting.\n",
              "\n",
              "2. **Highlights Key Patterns**:\n",
              "   - By transforming features into orthogonal components, PCA identifies and preserves relationships in the data that may not be obvious in raw features.\n",
              "\n",
              "3. **Improves Model Efficiency**:\n",
              "   - With fewer but more informative components, the model can train faster and generalize better, particularly on unseen data.\n",
              "\n",
              "---\n",
              "\n",
              "### **Impacts**\n",
              "1. **Enhanced Predictive Accuracy**:\n",
              "   - Eliminating irrelevant or redundant features improves the model's ability to make accurate predictions.\n",
              "\n",
              "2. **Improved Interpretability**:\n",
              "   - PCA simplifies the dataset, making it easier to understand the relationships driving the predictions.\n",
              "\n",
              "3. **Better Generalization**:\n",
              "   - Reduced dimensionality mitigates overfitting risks, ensuring consistent performance on validation and test datasets.\n",
              "\n",
              "\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='feature_engineering_3_explanations', value=feature_engineering_3_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8MNBrC4Zgz6"
      },
      "source": [
        "---\n",
        "## G. Train Machine Learning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_tQitOfeDXr"
      },
      "source": [
        "### G.1 Import Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh6epkAThez5"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOzgowlJ3uyp"
      },
      "outputs": [],
      "source": [
        "\n",
        "algorithm_selection_explanations = \"\"\"\n",
        "The selected algorithm, Logistic Regression, is an excellent fit for this project due to several reasons:\n",
        "\n",
        "### **Why It's a Good Fit**\n",
        "1. **Multiclass Classification Capability**:\n",
        "   - Logistic Regression can handle multiclass classification efficiently with the `multinomial` setting, making it ideal for predicting student performance categories (\"Excellent,\" \"Good,\" \"Average,\" \"Poor\").\n",
        "\n",
        "2. **Linear Relationships**:\n",
        "   - This algorithm excels at modeling linear relationships between features like `study_hours`, `social_media_hours`, and the target variable, ensuring interpretable results.\n",
        "\n",
        "3. **Scalability and Efficiency**:\n",
        "   - Logistic Regression is computationally efficient, allowing it to handle datasets like ours with scaled features, ensuring quick training and testing processes.\n",
        "\n",
        "4. **Robustness with Feature Selection**:\n",
        "   - The chosen features—such as `study_hours`, `previous_gpa`, and `social_media_hours`—align well with Logistic Regression's ability to weigh feature importance effectively.\n",
        "\n",
        "5. **Performance on Imbalanced Data**:\n",
        "   - With appropriate hyperparameter tuning (e.g., `max_iter=500`), Logistic Regression can be adapted to address class imbalances effectively, as shown in the weighted F1-score improvement during validation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "BYNgEFLX3u17",
        "outputId": "54ed080d-b3dc-42d5-c384-77d4e75e79a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"algorithm_selection_explanations\">\n",
              "The selected algorithm, Logistic Regression, is an excellent fit for this project due to several reasons:\n",
              "\n",
              "### **Why It's a Good Fit**\n",
              "1. **Multiclass Classification Capability**:\n",
              "   - Logistic Regression can handle multiclass classification efficiently with the `multinomial` setting, making it ideal for predicting student performance categories (\"Excellent,\" \"Good,\" \"Average,\" \"Poor\").\n",
              "\n",
              "2. **Linear Relationships**:\n",
              "   - This algorithm excels at modeling linear relationships between features like `study_hours`, `social_media_hours`, and the target variable, ensuring interpretable results.\n",
              "\n",
              "3. **Scalability and Efficiency**:\n",
              "   - Logistic Regression is computationally efficient, allowing it to handle datasets like ours with scaled features, ensuring quick training and testing processes.\n",
              "\n",
              "4. **Robustness with Feature Selection**:\n",
              "   - The chosen features—such as `study_hours`, `previous_gpa`, and `social_media_hours`—align well with Logistic Regression's ability to weigh feature importance effectively.\n",
              "\n",
              "5. **Performance on Imbalanced Data**:\n",
              "   - With appropriate hyperparameter tuning (e.g., `max_iter=500`), Logistic Regression can be adapted to address class imbalances effectively, as shown in the weighted F1-score improvement during validation.\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='algorithm_selection_explanations', value=algorithm_selection_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ks_MmM2mCfm"
      },
      "source": [
        "### G.2 Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUswpGVLmDXl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize Logistic Regression (multiclass by default using 'multinomial' solver)\n",
        "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_hjiXab31B3"
      },
      "outputs": [],
      "source": [
        "\n",
        "hyperparameters_selection_explanations = \"\"\"\n",
        "Tuning hyperparameters is essential for optimizing the performance of the Logistic Regression model. For instance, adjusting max_iter=500 ensures the model converges properly, especially when dealing with complex multiclass classification tasks. Choosing the multinomial option for the multi_class parameter is important since the goal is to classify student performance into multiple categories like \"Excellent,\" \"Good,\" \"Average,\" and \"Poor.\" The solver='lbfgs' is ideal for handling large datasets with high-dimensional features efficiently\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "tcZOBvcv31E5",
        "outputId": "b0f5ca82-914d-48ca-a14f-384d6c9dd118"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"hyperparameters_selection_explanations\">\n",
              "Tuning hyperparameters is essential for optimizing the performance of the Logistic Regression model. For instance, adjusting max_iter=500 ensures the model converges properly, especially when dealing with complex multiclass classification tasks. Choosing the multinomial option for the multi_class parameter is important since the goal is to classify student performance into multiple categories like \"Excellent,\" \"Good,\" \"Average,\" and \"Poor.\" The solver='lbfgs' is ideal for handling large datasets with high-dimensional features efficiently\n",
              "\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='hyperparameters_selection_explanations', value=hyperparameters_selection_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDjdjQjFmkLe"
      },
      "source": [
        "### G.3 Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ub3Nrdgmm2N",
        "outputId": "7c9b2e27-dc7a-4dbb-ca48-56ba4a308669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[62 22  4  0]\n",
            " [18 20  8  1]\n",
            " [ 0  4  7  0]\n",
            " [ 0  0  2  0]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.70      0.74        88\n",
            "         1.0       0.43      0.43      0.43        47\n",
            "         2.0       0.33      0.64      0.44        11\n",
            "         3.0       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.60       148\n",
            "   macro avg       0.39      0.44      0.40       148\n",
            "weighted avg       0.62      0.60      0.61       148\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Fit the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg.predict(X_val)\n",
        "# Evaluate the Model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_val, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q43YtqpdeniY"
      },
      "source": [
        "### G.4 Model Technical Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1Q3oxoNhez5",
        "outputId": "d10f5df5-403a-4a81-f7c6-0de055c2e5f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[54 10  6  0]\n",
            " [25 22  4  3]\n",
            " [ 2 10  5  1]\n",
            " [ 0  1  6  1]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.67      0.77      0.72        70\n",
            "         1.0       0.51      0.41      0.45        54\n",
            "         2.0       0.24      0.28      0.26        18\n",
            "         3.0       0.20      0.12      0.15         8\n",
            "\n",
            "    accuracy                           0.55       150\n",
            "   macro avg       0.40      0.40      0.39       150\n",
            "weighted avg       0.53      0.55      0.54       150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Predict on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "# Evaluate the Model\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7movz2AD3-k_"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_performance_explanations = \"\"\"\n",
        "### Explanation of Model Performance\n",
        "\n",
        "### **Key Observations**\n",
        "1. **Strength in Majority Classes**:\n",
        "   - The model performs well in predicting the \"Poor\" category (class \"0.0\") due to its dominance in the dataset, achieving high precision (68-75%) and recall (77-84%).\n",
        "\n",
        "2. **Challenges with Minority Classes**:\n",
        "   - For less frequent categories like \"Excellent\" (class \"3.0\"), the model fails completely with an F1-score and recall of 0%, indicating the need for additional balancing strategies or feature engineering.\n",
        "\n",
        "3. **Imbalance Sensitivity**:\n",
        "   - The model is heavily influenced by the dataset's class imbalance, which skews its ability to generalize across all performance categories.\n",
        "\n",
        "4. **Generalization Issues**:\n",
        "   - While validation accuracy is relatively decent at 65%, test accuracy drops to 51%, indicating the model might be overfitting to training data or failing to capture generalizable patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### **Implications**\n",
        "- These results indicate that while the model is effective for dominant classes, its inability to classify minority categories limits its practical use.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "1U_nKfW93-t0",
        "outputId": "36725561-2bc2-4c3a-9d0f-59393848e58e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"model_performance_explanations\">\n",
              "### Explanation of Model Performance\n",
              "\n",
              "### **Key Observations**\n",
              "1. **Strength in Majority Classes**:\n",
              "   - The model performs well in predicting the \"Poor\" category (class \"0.0\") due to its dominance in the dataset, achieving high precision (68-75%) and recall (77-84%).\n",
              "\n",
              "2. **Challenges with Minority Classes**:\n",
              "   - For less frequent categories like \"Excellent\" (class \"3.0\"), the model fails completely with an F1-score and recall of 0%, indicating the need for additional balancing strategies or feature engineering.\n",
              "\n",
              "3. **Imbalance Sensitivity**:\n",
              "   - The model is heavily influenced by the dataset's class imbalance, which skews its ability to generalize across all performance categories.\n",
              "\n",
              "4. **Generalization Issues**:\n",
              "   - While validation accuracy is relatively decent at 65%, test accuracy drops to 51%, indicating the model might be overfitting to training data or failing to capture generalizable patterns.\n",
              "\n",
              "---\n",
              "\n",
              "### **Implications**\n",
              "- These results indicate that while the model is effective for dominant classes, its inability to classify minority categories limits its practical use.\n",
              "\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='model_performance_explanations', value=model_performance_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1HgZMPcmtu7"
      },
      "source": [
        "### G.5 Business Impact from Current Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGq2RWyqmuKM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7zWIZ4v4Ehd"
      },
      "outputs": [],
      "source": [
        "\n",
        "business_impacts_explanations = \"\"\"The experimental results reveal that the Logistic Regression model performs moderately well in predicting the dominant \"Poor\" category (class \"0.0\") but struggles significantly with minority categories, particularly \"Excellent\" (class \"3.0\").\n",
        "\n",
        "### **Model Performance Summary**\n",
        "- **Validation Accuracy**: 65%\n",
        "- **Test Accuracy**: 51%\n",
        "- **Precision & Recall**:\n",
        "  - The model achieves acceptable precision (75%) and recall (84%) for the \"Poor\" category, but fails for \"Excellent,\" with both metrics at 0%.\n",
        "\n",
        "### **Business Impact**\n",
        "1. **Positive Impact of Correct Results**:\n",
        "   - For the \"Poor\" category, accurate predictions enable early interventions, improving student success rates and resource allocation efficiency.\n",
        "\n",
        "2. **Negative Impact of Incorrect Results**:\n",
        "   - Misclassification in the \"Poor\" category leads to missed opportunities for aiding at-risk students, directly affecting academic outcomes.\n",
        "   - For \"Excellent\" and \"Good\" categories, low recall and precision result in an inability to identify high-performing students, potentially impacting recognition and support programs. These errors are less critical compared to misclassifications in the \"Poor\" category but still affect institutional reputation and trust.\n",
        "\n",
        "3. **Overall Impact**:\n",
        "   - The skewed performance highlights the need for addressing imbalances to ensure equitable treatment of all categories.\n",
        "   - Incorrect predictions in minority classes can lead to systemic biases, hampering the effectiveness of resource allocation and support strategies.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "OfuoUsp84Eod",
        "outputId": "bc2e6186-d307-4245-d630-8086b40c9978"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h3 id=\"business_impacts_explanations\">The experimental results reveal that the Logistic Regression model performs moderately well in predicting the dominant \"Poor\" category (class \"0.0\") but struggles significantly with minority categories, particularly \"Excellent\" (class \"3.0\").\n",
              "\n",
              "### **Model Performance Summary**\n",
              "- **Validation Accuracy**: 65%\n",
              "- **Test Accuracy**: 51%\n",
              "- **Precision & Recall**:\n",
              "  - The model achieves acceptable precision (75%) and recall (84%) for the \"Poor\" category, but fails for \"Excellent,\" with both metrics at 0%.\n",
              "\n",
              "### **Business Impact**\n",
              "1. **Positive Impact of Correct Results**:\n",
              "   - For the \"Poor\" category, accurate predictions enable early interventions, improving student success rates and resource allocation efficiency.\n",
              "\n",
              "2. **Negative Impact of Incorrect Results**:\n",
              "   - Misclassification in the \"Poor\" category leads to missed opportunities for aiding at-risk students, directly affecting academic outcomes.\n",
              "   - For \"Excellent\" and \"Good\" categories, low recall and precision result in an inability to identify high-performing students, potentially impacting recognition and support programs. These errors are less critical compared to misclassifications in the \"Poor\" category but still affect institutional reputation and trust.\n",
              "\n",
              "3. **Overall Impact**:\n",
              "   - The skewed performance highlights the need for addressing imbalances to ensure equitable treatment of all categories.\n",
              "   - Incorrect predictions in minority classes can lead to systemic biases, hampering the effectiveness of resource allocation and support strategies.\n",
              "\n",
              "</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h3\", key='business_impacts_explanations', value=business_impacts_explanations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp1Ie9o8nDl1"
      },
      "source": [
        "## H. Experiment Outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tw5lZ2S4KV3"
      },
      "outputs": [],
      "source": [
        "\n",
        "experiment_results_explanations = \"\"\"### Reflection on Experiment Outcomes\n",
        "\n",
        "#### **Experiment Outcome**\n",
        "The experiment resulted in a **partial confirmation of the hypothesis**. While the model demonstrated strength in predicting the \"Poor\" category (class \"0.0\") with a validation accuracy of **65%**, it struggled with minority classes, particularly \"Excellent\" (class \"3.0\"), where precision and recall values were both **0%**. Test accuracy was lower at **51%**, indicating potential overfitting or insufficient generalization.\n",
        "\n",
        "#### **New Insights Gained**\n",
        "1. **Imbalanced Data Challenge**:\n",
        "   - The model performed well for the majority \"Poor\" class but failed to generalize predictions for smaller classes, highlighting the need to address imbalanced data distribution.\n",
        "\n",
        "2. **Feature Relevance**:\n",
        "   - Features like `previous_gpa` (correlation 0.688) and `social_media_impact` (correlation 0.465) proved critical, confirming their strong predictive power.\n",
        "\n",
        "3. **Algorithm Limitations**:\n",
        "   - Logistic Regression's linear assumptions limited its ability to capture complex patterns, especially for diverse performance categories.\n",
        "\n",
        "#### **Rationale for Pursuing Further Experimentation**\n",
        "The current approach shows promise but requires refinement. The model aligns with the business objective of identifying at-risk students. Further experimentation is justified because incremental improvements—such as addressing class imbalance and exploring nonlinear models—can significantly boost predictive accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **Potential Next Steps and Experiments**\n",
        "1. **Address Class Imbalance**\n",
        "   - **Technique**: Apply SMOTE (Synthetic Minority Over-sampling Technique) or class-weight adjustments.\n",
        "   - **Expected Uplift**: Higher recall and precision for minority categories, particularly \"Excellent.\"\n",
        "   - **Priority**: **High**.\n",
        "\n",
        "2. **Explore Nonlinear Algorithms**\n",
        "   - **Technique**: Test Random Forests or XGBoost, which handle nonlinear relationships and class imbalances better.\n",
        "   - **Expected Uplift**: Improved accuracy across all categories, especially for minority classes.\n",
        "   - **Priority**: **High**.\n",
        "\n",
        "3. **Advanced Feature Engineering**\n",
        "   - **Technique**: Add interaction terms or polynomial features to capture non-linear relationships.\n",
        "   - **Expected Uplift**: Enhanced model performance by incorporating complex patterns.\n",
        "   - **Priority**: **Medium**.\n",
        "\n",
        "4. **Hyperparameter Tuning**\n",
        "   - **Technique**: Use grid search or Bayesian optimization to identify optimal settings for Logistic Regression or new models.\n",
        "   - **Expected Uplift**: Fine-tuned performance improvements in predictive metrics.\n",
        "   - **Priority**: **Medium**.\n",
        "\n",
        "5. **Dimensionality Reduction**\n",
        "   - **Technique**: Employ Principal Component Analysis (PCA) to simplify the dataset and reduce overfitting.\n",
        "   - **Expected Uplift**: Better generalization to test data and faster model training.\n",
        "   - **Priority**: **Low**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Recommendation for Deployment**\n",
        "If further experimentation achieves an accuracy of **≥75%** and balanced metrics across all categories, the model can be deployed. Deployment steps include:\n",
        "1. **Monitoring System**: Establish a framework to track predictions and performance metrics in real-time.\n",
        "2. **Documentation**: Provide guidelines for interpreting predictions and integrating them into academic workflows.\n",
        "3. **Stakeholder Training**: Ensure staff understand how to utilize the model for targeted interventions.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "H-Zd7BEz4Kc3",
        "outputId": "332b3ac0-e09d-465c-cd1f-46d1c06fcd95"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h2 id=\"experiment_results_explanations\">### Reflection on Experiment Outcomes\n",
              "\n",
              "#### **Experiment Outcome**\n",
              "The experiment resulted in a **partial confirmation of the hypothesis**. While the model demonstrated strength in predicting the \"Poor\" category (class \"0.0\") with a validation accuracy of **65%**, it struggled with minority classes, particularly \"Excellent\" (class \"3.0\"), where precision and recall values were both **0%**. Test accuracy was lower at **51%**, indicating potential overfitting or insufficient generalization.\n",
              "\n",
              "#### **New Insights Gained**\n",
              "1. **Imbalanced Data Challenge**:\n",
              "   - The model performed well for the majority \"Poor\" class but failed to generalize predictions for smaller classes, highlighting the need to address imbalanced data distribution.\n",
              "\n",
              "2. **Feature Relevance**:\n",
              "   - Features like `previous_gpa` (correlation 0.688) and `social_media_impact` (correlation 0.465) proved critical, confirming their strong predictive power.\n",
              "\n",
              "3. **Algorithm Limitations**:\n",
              "   - Logistic Regression's linear assumptions limited its ability to capture complex patterns, especially for diverse performance categories.\n",
              "\n",
              "#### **Rationale for Pursuing Further Experimentation**\n",
              "The current approach shows promise but requires refinement. The model aligns with the business objective of identifying at-risk students. Further experimentation is justified because incremental improvements—such as addressing class imbalance and exploring nonlinear models—can significantly boost predictive accuracy.\n",
              "\n",
              "---\n",
              "\n",
              "### **Potential Next Steps and Experiments**\n",
              "1. **Address Class Imbalance**\n",
              "   - **Technique**: Apply SMOTE (Synthetic Minority Over-sampling Technique) or class-weight adjustments.\n",
              "   - **Expected Uplift**: Higher recall and precision for minority categories, particularly \"Excellent.\"\n",
              "   - **Priority**: **High**.\n",
              "\n",
              "2. **Explore Nonlinear Algorithms**\n",
              "   - **Technique**: Test Random Forests or XGBoost, which handle nonlinear relationships and class imbalances better.\n",
              "   - **Expected Uplift**: Improved accuracy across all categories, especially for minority classes.\n",
              "   - **Priority**: **High**.\n",
              "\n",
              "3. **Advanced Feature Engineering**\n",
              "   - **Technique**: Add interaction terms or polynomial features to capture non-linear relationships.\n",
              "   - **Expected Uplift**: Enhanced model performance by incorporating complex patterns.\n",
              "   - **Priority**: **Medium**.\n",
              "\n",
              "4. **Hyperparameter Tuning**\n",
              "   - **Technique**: Use grid search or Bayesian optimization to identify optimal settings for Logistic Regression or new models.\n",
              "   - **Expected Uplift**: Fine-tuned performance improvements in predictive metrics.\n",
              "   - **Priority**: **Medium**.\n",
              "\n",
              "5. **Dimensionality Reduction**\n",
              "   - **Technique**: Employ Principal Component Analysis (PCA) to simplify the dataset and reduce overfitting.\n",
              "   - **Expected Uplift**: Better generalization to test data and faster model training.\n",
              "   - **Priority**: **Low**.\n",
              "\n",
              "---\n",
              "\n",
              "### **Recommendation for Deployment**\n",
              "If further experimentation achieves an accuracy of **≥75%** and balanced metrics across all categories, the model can be deployed. Deployment steps include:\n",
              "1. **Monitoring System**: Establish a framework to track predictions and performance metrics in real-time.\n",
              "2. **Documentation**: Provide guidelines for interpreting predictions and integrating them into academic workflows.\n",
              "3. **Stakeholder Training**: Ensure staff understand how to utilize the model for targeted interventions.\n",
              "\n",
              "\n",
              "</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this code\n",
        "print_tile(size=\"h2\", key='experiment_results_explanations', value=experiment_results_explanations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWPTMmkxTbwy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_tIgXuiRqIs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "student-at-risk-detector-py3.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
